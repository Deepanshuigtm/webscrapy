# webscrapy
filtering the posts that have more than 99 votes on the Hacker News website. Hacker News is a social news website where users submit and vote on articles. The website is built using HTML and CSS, and we will be using Beautiful Soup to parse and extract the relevant information from the HTML.

you need to install beautifulsoup into your machine you can use pip install beautifulsoup into you terminal if you mac user and it will install beautiful soup for you
it is important to check whether the website allows web scraping or not. Many websites do not want their data to be scraped and they may have certain measures in place to prevent it.
tne way to check if a website allows scraping is to look for a file called robots.txt. This file is placed on the root of the website and contains information on which parts of the website can be crawled and which cannot. It is a voluntary protocol that websites follow to communicate with web crawlers and other automated agents.
By looking at the robots.txt file, you can see which parts of the website are disallowed and which parts are allowed. For example, if you visit "https://news.ycombinator.com/robots.txt", you can see that the file allows web crawlers to access the main page of the website, but disallows certain pages such as the login page.
